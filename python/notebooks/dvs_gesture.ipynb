{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c5d5ed",
   "metadata": {},
   "source": [
    "# DVS Gesture Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49091063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch.functional import quant\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "# Quantization\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Tonic\n",
    "import tonic\n",
    "from tonic import DiskCachedDataset\n",
    "from tonic import MemoryCachedDataset\n",
    "from tonic.transforms import Compose, ToFrame, Downsample\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import pyfenrir as fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869af1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "batch_size = 32\n",
    "frame_length_us = 16.6e3\n",
    "target_size = (80, 60)\n",
    "n_timesteps = 180\n",
    "\n",
    "def pad_time_dimension(frames, fixed_time_steps=100):\n",
    "    \"\"\"\n",
    "    Pad or truncate the time dimension of frames to a fixed number of time steps.\n",
    "    Input: frames [time, channels, height, width] (numpy or tensor)\n",
    "    Output: frames [fixed_time_steps, channels, height, width] (tensor)\n",
    "    \"\"\"\n",
    "    # Convert to tensor if input is numpy array\n",
    "    if isinstance(frames, np.ndarray):\n",
    "        frames = torch.tensor(frames, dtype=torch.float)\n",
    "    current_time_steps = frames.shape[0]\n",
    "    #print(f\"Current time steps: {current_time_steps}, Fixed time steps: {fixed_time_steps}\")\n",
    "    if current_time_steps > fixed_time_steps:\n",
    "        return frames[:fixed_time_steps]\n",
    "    elif current_time_steps < fixed_time_steps:\n",
    "        return torch.nn.functional.pad(frames, (0, 0, 0, 0, 0, 0, 0, fixed_time_steps - current_time_steps))\n",
    "    return frames\n",
    "\n",
    "transform = Compose([\n",
    "    Downsample(sensor_size=sensor_size, target_size=target_size),\n",
    "    ToFrame(sensor_size=(target_size[0], target_size[1], sensor_size[2]), time_window=frame_length_us),\n",
    "    transforms.Lambda(lambda x: pad_time_dimension(x, fixed_time_steps=n_timesteps)),   # Pad/truncate time dimension\n",
    "    transforms.Lambda(lambda x: x[:, 1:2, :, :]  ),                                       # Select only ON channel\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../data', train=True, transform=transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../data', train=False, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(trainset.targets))\n",
    "\n",
    "print(f\"The DVSGesture dataset has {num_classes} target classes.\")\n",
    "\n",
    "test_data = trainset[0][0]\n",
    "print(f\"Shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "if isinstance(test_data, torch.Tensor):\n",
    "    frames_np = test_data.cpu().numpy()\n",
    "else:\n",
    "    frames_np = test_data\n",
    "\n",
    "print(f\"Shape of frames for video (flattened): {frames_np.shape}\")\n",
    "print(f\"Number of frames: {frames_np.shape[0]}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.set_title(\"Positive Events Animation\")\n",
    "ax.axis('off')\n",
    "\n",
    "vmax_val = np.max(frames_np) if frames_np.size > 0 else 1\n",
    "\n",
    "# Use the first frame and squeeze channel dimension for imshow\n",
    "im = ax.imshow(frames_np[0, 0, :, :], cmap='gray_r', vmin=0, vmax=vmax_val)\n",
    "\n",
    "def update(frame):\n",
    "    # Display the frame by squeezing out channel dimension\n",
    "    im.set_data(frames_np[frame, 0, :, :])\n",
    "    ax.set_title(f\"Frame: {frame+1}/{frames_np.shape[0]}\")\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=frames_np.shape[0],\n",
    "    interval=frame_length_us / 1000,\n",
    "    blit=True\n",
    ")\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39620c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Training\n",
    "    \"num_epochs\": 10,           # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": batch_size,   # Batch size\n",
    "    \"seed\": 0,                  # Random seed\n",
    "\n",
    "    # Data\n",
    "    \"num_steps\": 100,           # Number of timesteps to encode input for\n",
    "    \"num_classes\": 11,          # Number of classes\n",
    "    \"width\": 80,                # Sensor width\n",
    "    \"height\": 60,               # Sensor height\n",
    "    \n",
    "    # Quantization\n",
    "    \"fc1_bits\": 4,              # Bits per weight for fc1\n",
    "    \"fc2_bits\": 4,              # Bits per weight for fc2\n",
    "    \n",
    "    # Conv parameters\n",
    "    \"conv1_out\": 12,            # Output channels for conv1\n",
    "    \"conv2_out\": 5,            # Output channels for conv1\n",
    "    \"conv3_out\": 5,             # Output channels for conv1\n",
    "    \"kernel_size\": 3,           # Kernel size for all conv layers  \n",
    "\n",
    "    # Fully-connected parameters\n",
    "    \"fc1_beta\": 0.1,            # Initial decay rate for lif1\n",
    "    \"fc2_beta\": 1.0,            # Initial decay rate for lif2 \n",
    "    \"fc1_thr\": 1.0,             # Initial threshold for lif1\n",
    "    \"fc2_thr\": 1.0,             # Initial threshold for lif2\n",
    "    \"fc1_multiplier\": 10,       # Weight multiplier for fc1\n",
    "    \"fc2_multiplier\": 10,       # Weight multiplier for fc2\n",
    "\n",
    "    # Learning\n",
    "    \"lr\": 3.0e-3,               # Initial learning rate\n",
    "    \"slope\": 5.6,               # Slope value (k)\n",
    "    \n",
    "    # Fixed params\n",
    "    \"correct_rate\": 0.8,        # Correct rate (loss function)\n",
    "    \"incorrect_rate\": 0.2,      # Incorrect rate (loss function)\n",
    "    \"betas\": (0.9, 0.999),      # Adam optimizer beta values\n",
    "    \"t_0\": 4690,                # Initial frequency of the cosine annealing scheduler\n",
    "    \"eta_min\": 0,               # Minimum learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = fnr.FenrirNet(config).to(device)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    betas=config[\"betas\"]\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"t_0\"],\n",
    "    eta_min=config[\"eta_min\"],\n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "criterion = SF.mse_count_loss(\n",
    "    correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ea85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=\"cpu\", scheduler=None, current_epoch=0, grad_log_file=None):\n",
    "    \"\"\"\n",
    "    Complete one epoch of training.\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    lr_accum = []\n",
    "    \n",
    "    pbar = tqdm(trainloader, leave=False, desc=f\"Epoch {current_epoch} Training\")\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(pbar):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        spk_rec = net(data)\n",
    "        loss = criterion(spk_rec, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        batch_accuracy_percent = 0.0\n",
    "        current_loss_val = loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_accuracy = SF.accuracy_rate(spk_rec, labels)\n",
    "            \n",
    "            if isinstance(batch_accuracy, torch.Tensor):\n",
    "                batch_accuracy = batch_accuracy.item()\n",
    "            batch_accuracy_percent = batch_accuracy * 100\n",
    "\n",
    "        total_grad_norm = 0\n",
    "        for p in net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm ** 0.5\n",
    "\n",
    "        postfix_dict = {\n",
    "            \"loss\": f\"{current_loss_val:.4f}\",\n",
    "            \"acc\": f\"{batch_accuracy_percent:.2f}%\",\n",
    "            \"grad_norm\": f\"{total_grad_norm:.2e}\"\n",
    "        }\n",
    "        pbar.set_postfix(postfix_dict)\n",
    "        \n",
    "        loss_to_accumulate = current_loss_val / config[\"num_steps\"]\n",
    "        loss_accum.append(loss_to_accumulate)\n",
    "        \n",
    "        lr_accum.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # === Gradient Logging to File ===\n",
    "        if batch_idx == 0 and grad_log_file is not None:\n",
    "            try:\n",
    "                with open(grad_log_file, 'a') as f_log: # Open in append mode\n",
    "                    f_log.write(f\"\\nEpoch {current_epoch}, Batch {batch_idx} - Gradient Norms (Detailed):\\n\")\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            grad_norm_val = param.grad.norm().item()\n",
    "                            f_log.write(f\"  {name:30}: {grad_norm_val:.6f}\\n\")\n",
    "                        else:\n",
    "                            f_log.write(f\"  {name:30}: No gradient\\n\")\n",
    "                    f_log.write(\"-\" * 50 + \"\\n\")\n",
    "            except IOError as e:\n",
    "                print(f\"Warning: Could not write to gradient log file {grad_log_file}. Error: {e}\")\n",
    "\n",
    "    return loss_accum, lr_accum\n",
    "\n",
    "def test(config, net, testloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculate accuracy on full test set.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            accuracy = SF.accuracy_rate(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "loss_list = []\n",
    "lr_list = []\n",
    "\n",
    "GRADIENT_LOG_FILE = \"gradient_norms_log.txt\"\n",
    "\n",
    "## Load model instead of training\n",
    "load_model = False\n",
    "if load_model:\n",
    "    net.load_state_dict(torch.load('../models/nmnist_1layer.pth'))\n",
    "else:\n",
    "    print(f\"=======Training Network=======\")\n",
    "    with open(GRADIENT_LOG_FILE, 'w') as f_log:\n",
    "        f_log.write(\"Gradient Norm Log\\n\")\n",
    "        f_log.write(\"====================\\n\")\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        loss, lr  = train(config, net, trainloader, criterion, optimizer, device, scheduler, \n",
    "                          current_epoch=epoch, grad_log_file=GRADIENT_LOG_FILE)\n",
    "        loss_list = loss_list + loss\n",
    "        lr_list   = lr_list + lr\n",
    "        # Test\n",
    "        test_accuracy = test(config, net, testloader, device)\n",
    "        print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy}\")\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(loss_list, color='tab:orange')\n",
    "    ax2.plot(lr_list, color='tab:blue')\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss', color='tab:orange')\n",
    "    ax2.set_ylabel('Learning Rate', color='tab:blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model:\n",
    "    dir = \"../models\"\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    torch.save(net.state_dict(), f\"{dir}/gesture_fenrir_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fenrir.export_weights(net.fc1, 44, 32*24, '../../src/design_sources/data/fc1_gesture.data')\n",
    "quant_scale = net.fc1.quant_weight().scale\n",
    "beta        = net.fc1_beta/net.fc1.quant_weight().scale\n",
    "thr         = fnr.get_threshold(net.fc1, net.lif1)\n",
    "print(f\"Quant Scale: {quant_scale}\")\n",
    "print(f\"Beta: {beta}\")\n",
    "print(f\"Threshold: {thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fenrir.export_weights(net.fc1, 44, 32*24, '../../src/design_sources/data/fc1_gesture.data')\n",
    "quant_scale = net.fc2.quant_weight().scale\n",
    "beta        = net.fc2_beta/net.fc2.quant_weight().scale\n",
    "thr         = fnr.get_threshold(net.fc2, net.lif2)\n",
    "print(f\"Quant Scale: {quant_scale}\")\n",
    "print(f\"Beta: {beta}\")\n",
    "print(f\"Threshold: {thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f4ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvs-fpga-_hAg3Ylq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
