{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01df6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_weights_to_conv2d(weights_file):\n",
    "    \"\"\"\n",
    "    Load kernel weights and create a PyTorch Conv2d layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weights_file : str\n",
    "        Path to weights file (.json, .npy, or .mem)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    conv_layer : nn.Conv2d\n",
    "        PyTorch convolution layer with loaded weights\n",
    "    \"\"\"\n",
    "    weights_file = Path(weights_file)\n",
    "    \n",
    "    if weights_file.suffix == \".json\":\n",
    "        with open(weights_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        weights = np.array(data['weights'], dtype=np.float32)\n",
    "        config = data['config']\n",
    "        \n",
    "    elif weights_file.suffix == \".npy\":\n",
    "        weights = np.load(weights_file).astype(np.float32)\n",
    "        # Infer config from shape\n",
    "        in_channels, out_channels, kernel_size, _ = weights.shape\n",
    "        config = {\n",
    "            'in_channels': in_channels,\n",
    "            'out_channels': out_channels,\n",
    "            'kernel_size': kernel_size\n",
    "        }\n",
    "        \n",
    "    elif weights_file.suffix == \".mem\":\n",
    "        # Parse Vivado memory file\n",
    "        config = {}\n",
    "        data_lines = []\n",
    "        \n",
    "        with open(weights_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('// Input channels:'):\n",
    "                    config['in_channels'] = int(line.split(':')[1].strip())\n",
    "                elif line.startswith('// Output channels:'):\n",
    "                    config['out_channels'] = int(line.split(':')[1].strip())\n",
    "                elif line.startswith('// Kernel size:'):\n",
    "                    kernel_info = line.split(':')[1].strip()\n",
    "                    kernel_size = int(kernel_info.split('x')[0])\n",
    "                    config['kernel_size'] = kernel_size\n",
    "                elif line.startswith('// Bits per weight:'):\n",
    "                    config['bits_per_weight'] = int(line.split(':')[1].strip())\n",
    "                elif line.startswith('@') and not line.startswith('//'):\n",
    "                    data_lines.append(line)\n",
    "        \n",
    "        # Reconstruct weights from hex data\n",
    "        in_channels = config['in_channels']\n",
    "        out_channels = config['out_channels']\n",
    "        kernel_size = config['kernel_size']\n",
    "        bits_per_weight = config['bits_per_weight']\n",
    "        \n",
    "        weights = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.int32)\n",
    "        \n",
    "        for line in data_lines:\n",
    "            parts = line.split()\n",
    "            address = int(parts[0][1:], 16)\n",
    "            hex_data = parts[1]\n",
    "            packed_value = int(hex_data, 16)\n",
    "            \n",
    "            # Decode address\n",
    "            total_positions = kernel_size * kernel_size\n",
    "            in_ch = address // total_positions\n",
    "            pos = address % total_positions\n",
    "            row = pos // kernel_size\n",
    "            col = pos % kernel_size\n",
    "            \n",
    "            # Unpack weights\n",
    "            for out_ch in range(out_channels):\n",
    "                shift = out_ch * bits_per_weight\n",
    "                mask = (1 << bits_per_weight) - 1\n",
    "                weight_unsigned = (packed_value >> shift) & mask\n",
    "                \n",
    "                # Convert to signed\n",
    "                if weight_unsigned >= (1 << (bits_per_weight - 1)):\n",
    "                    weight_signed = weight_unsigned - (1 << bits_per_weight)\n",
    "                else:\n",
    "                    weight_signed = weight_unsigned\n",
    "                    \n",
    "                weights[in_ch, out_ch, row, col] = weight_signed\n",
    "        \n",
    "        weights = weights.astype(np.float32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {weights_file.suffix}\")\n",
    "    \n",
    "    # Convert to PyTorch format: [out_channels, in_channels, height, width]\n",
    "    torch_weights = torch.from_numpy(weights).permute(1, 0, 2, 3)\n",
    "    \n",
    "    # Create Conv2d layer\n",
    "    conv_layer = nn.Conv2d(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        kernel_size=config['kernel_size'],\n",
    "        stride=1,\n",
    "        padding=config['kernel_size'] // 2,\n",
    "        bias=False\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    with torch.no_grad():\n",
    "        conv_layer.weight.copy_(torch_weights)\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "def load_events(events_file):\n",
    "    \"\"\"\n",
    "    Load events and return coordinates and channels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    events_file : str\n",
    "        Path to events file (.json or .mem)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    events : np.ndarray\n",
    "        Array of shape (n_events, 4) with columns [x, y, channels, timestep]\n",
    "        where channels is the integer spike pattern\n",
    "    \"\"\"\n",
    "    events_file = Path(events_file)\n",
    "    \n",
    "    if events_file.suffix == \".json\":\n",
    "        with open(events_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        events_list = data['events']\n",
    "        \n",
    "        events = []\n",
    "        for event in events_list:\n",
    "            events.append([\n",
    "                event['x'],\n",
    "                event['y'], \n",
    "                event['spikes'],\n",
    "                event['timestep']\n",
    "            ])\n",
    "        \n",
    "        return np.array(events, dtype=np.int32)\n",
    "    \n",
    "    elif events_file.suffix == \".mem\":\n",
    "        # Parse binary event file\n",
    "        events = []\n",
    "        \n",
    "        with open(events_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('//'):\n",
    "                    # Extract binary data (before any comment)\n",
    "                    binary_str = line.split()[0]\n",
    "                    \n",
    "                    # Parse based on your format: [timestep][x][y][spikes]\n",
    "                    # You'll need to specify bit widths - assuming from your testbench:\n",
    "                    bits_per_coord = 7  # Adjust as needed\n",
    "                    in_channels = 4     # Adjust as needed\n",
    "                    \n",
    "                    total_bits = len(binary_str)\n",
    "                    \n",
    "                    # Extract fields\n",
    "                    timestep = int(binary_str[0])\n",
    "                    x_bits = binary_str[1:1+bits_per_coord]\n",
    "                    y_bits = binary_str[1+bits_per_coord:1+2*bits_per_coord]\n",
    "                    spikes_bits = binary_str[1+2*bits_per_coord:]\n",
    "                    \n",
    "                    x = int(x_bits, 2)\n",
    "                    y = int(y_bits, 2)\n",
    "                    spikes = int(spikes_bits, 2)\n",
    "                    \n",
    "                    events.append([x, y, spikes, timestep])\n",
    "        \n",
    "        return np.array(events, dtype=np.int32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {events_file.suffix}\")\n",
    "def load_memory_dumps(dump_directory=\".\", bits_per_neuron=9, out_channels=4, pattern=\"feature_map_mem_*.mem\"):\n",
    "    \"\"\"\n",
    "    Load Vivado memory dumps and return signed membrane potential values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dump_directory : str\n",
    "        Directory containing memory dump files\n",
    "    bits_per_neuron : int\n",
    "        Number of bits per neuron value\n",
    "    out_channels : int\n",
    "        Number of output channels (neurons per memory location)\n",
    "    pattern : str\n",
    "        Filename pattern for memory dumps\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    memory_data : list of np.ndarray\n",
    "        List of memory dumps, each with shape (memory_depth, out_channels)\n",
    "        containing signed membrane potential values\n",
    "    dump_files : list of str\n",
    "        List of dump filenames in order\n",
    "    \"\"\"\n",
    "    # Find dump files\n",
    "    search_pattern = str(Path(dump_directory) / pattern)\n",
    "    dump_files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not dump_files:\n",
    "        raise FileNotFoundError(f\"No memory dump files found: {search_pattern}\")\n",
    "    \n",
    "    # Sort by dump number\n",
    "    def extract_dump_number(filename):\n",
    "        match = re.search(r'feature_map_mem_(\\d+)\\.mem', Path(filename).name)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    dump_files.sort(key=extract_dump_number)\n",
    "    \n",
    "    memory_data = []\n",
    "    \n",
    "    for dump_file in dump_files:\n",
    "        with open(dump_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Remove comments and empty lines\n",
    "        mem_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('//'):\n",
    "                mem_lines.append(line)\n",
    "        \n",
    "        # Parse each memory line into multiple neuron values\n",
    "        dump_array = []\n",
    "        for mem_line in mem_lines:\n",
    "            if len(mem_line) > 0:\n",
    "                # Each memory line contains out_channels packed neurons\n",
    "                neuron_values = []\n",
    "                \n",
    "                for ch in range(out_channels):\n",
    "                    # Extract bits for this channel\n",
    "                    start_bit = ch * bits_per_neuron\n",
    "                    end_bit = start_bit + bits_per_neuron\n",
    "                    \n",
    "                    if end_bit <= len(mem_line):\n",
    "                        # Extract binary substring for this neuron\n",
    "                        neuron_bits = mem_line[start_bit:end_bit]\n",
    "                        \n",
    "                        # Convert to signed integer using two's complement\n",
    "                        unsigned_val = int(neuron_bits, 2)\n",
    "                        if unsigned_val >= (1 << (bits_per_neuron - 1)):\n",
    "                            signed_val = unsigned_val - (1 << bits_per_neuron)\n",
    "                        else:\n",
    "                            signed_val = unsigned_val\n",
    "                        \n",
    "                        neuron_values.append(signed_val)\n",
    "                    else:\n",
    "                        # Handle case where memory line is shorter than expected\n",
    "                        neuron_values.append(0)\n",
    "                \n",
    "                dump_array.append(neuron_values)\n",
    "        \n",
    "        memory_data.append(np.array(dump_array, dtype=np.int32))\n",
    "    \n",
    "    return memory_data, [Path(f).name for f in dump_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92ca356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_weights_path = r'C:\\Users\\alext\\fenrir\\python\\conv_test_scripts\\kernel_weights.json'\n",
    "events_path = r'C:\\Users\\alext\\fenrir\\python\\conv_test_scripts\\snn_test_events.json'\n",
    "memory_dumps_path = r'E:\\rtsprojects\\general_conv\\general_conv.sim\\test_all\\behav\\xsim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e907780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 64, 4), (64, 4), array([ 0,  0,  0, -2]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_dumps, dump_files = load_memory_dumps(memory_dumps_path, bits_per_neuron=9)\n",
    "memory_dumps = (np.array(memory_dumps))\n",
    "memory_dumps.shape, memory_dumps[0].shape, memory_dumps[0][49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cbc088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = load_weights_to_conv2d(kernel_weights_path)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9a5676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 2, 9, 0],\n",
       "       [2, 7, 1, 0],\n",
       "       [1, 6, 8, 0],\n",
       "       [7, 0, 4, 0],\n",
       "       [2, 0, 8, 0],\n",
       "       [1, 7, 5, 0],\n",
       "       [0, 7, 1, 0],\n",
       "       [7, 1, 8, 0],\n",
       "       [0, 7, 0, 0],\n",
       "       [4, 7, 3, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = load_events(events_path)\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8adfd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1\n",
    "bit_string = format(n, '04b')\n",
    "bit_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-conv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
