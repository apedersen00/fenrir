{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c5d5ed",
   "metadata": {},
   "source": [
    "# DVS Gesture Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49091063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch.functional import quant\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "# Quantization\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Tonic\n",
    "import tonic\n",
    "from tonic import DiskCachedDataset\n",
    "from tonic import MemoryCachedDataset\n",
    "from tonic.transforms import Compose, ToFrame, Downsample\n",
    "from tonic import transforms as tonic_transforms\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import pyfenrir as fenrir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869af1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "batch_size = 64\n",
    "frame_length_us = 16.6e3\n",
    "target_size = (60, 60)\n",
    "n_timesteps = 180\n",
    "\n",
    "def pad_time_dimension(frames, fixed_time_steps=100):\n",
    "    \"\"\"\n",
    "    Pad or truncate the time dimension of frames to a fixed number of time steps.\n",
    "    Input: frames [time, channels, height, width] (numpy or tensor)\n",
    "    Output: frames [fixed_time_steps, channels, height, width] (tensor)\n",
    "    \"\"\"\n",
    "    # Convert to tensor if input is numpy array\n",
    "    if isinstance(frames, np.ndarray):\n",
    "        frames = torch.tensor(frames, dtype=torch.float)\n",
    "    current_time_steps = frames.shape[0]\n",
    "    #print(f\"Current time steps: {current_time_steps}, Fixed time steps: {fixed_time_steps}\")\n",
    "    if current_time_steps > fixed_time_steps:\n",
    "        return frames[:fixed_time_steps]\n",
    "    elif current_time_steps < fixed_time_steps:\n",
    "        return torch.nn.functional.pad(frames, (0, 0, 0, 0, 0, 0, 0, fixed_time_steps - current_time_steps))\n",
    "    return frames\n",
    "\n",
    "transform = Compose([\n",
    "    tonic_transforms.Downsample(sensor_size=(128, 128), target_size=(60, 60)),\n",
    "    tonic_transforms.ToFrame(sensor_size=(60, 60, 128), time_window=frame_length_us),\n",
    "    transforms.Lambda(lambda x: pad_time_dimension(x, fixed_time_steps=n_timesteps)),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1).type(torch.float32)),\n",
    "    transforms.Lambda(lambda x: x[:, 1:2, :, :])  # Select only ON channel (index 1)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../data', train=True, transform=transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../data', train=False, transform=transform)\n",
    "\n",
    "cached_trainset = MemoryCachedDataset(trainset)\n",
    "cached_testset = MemoryCachedDataset(testset)\n",
    "\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "testloader = DataLoader(cached_testset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(trainset.targets))\n",
    "\n",
    "print(f\"The DVSGesture dataset has {num_classes} target classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetUtils():\n",
    "    @staticmethod\n",
    "    def beta_clamp(mem, beta):\n",
    "        \"\"\"\n",
    "        Soft-clamping of beta to allow gradients.\n",
    "        \"\"\"\n",
    "        beta_abs = torch.abs(beta)\n",
    "        # Positive side: approximate clamp(mem - beta_abs, min=0)\n",
    "        pos_mask = (mem > 0)\n",
    "        pos_val = F.relu(mem - beta_abs)  # ReLU is differentiable everywhere except 0 (and better than clamp)\n",
    "\n",
    "        # Negative side: approximate clamp(mem + beta_abs, max=0)\n",
    "        neg_mask = (mem < 0)\n",
    "        neg_val = -F.relu(-(mem + beta_abs))  # negative ReLU for negative side\n",
    "\n",
    "        mem_new = torch.where(pos_mask, pos_val, mem)\n",
    "        mem_new = torch.where(neg_mask, neg_val, mem_new)\n",
    "\n",
    "        return mem_new\n",
    "\n",
    "    @staticmethod\n",
    "    def mem_clamp(mem, scale, multiplier, bits=12):\n",
    "        max_val = (2**(bits - 1)) - 1\n",
    "        max_val = max_val * scale / multiplier\n",
    "        min_val = -(2**(bits - 1)) - 1\n",
    "        min_val = min_val * scale / multiplier\n",
    "        mem = torch.clamp(mem, min=min_val, max=max_val)\n",
    "        return mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffde9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_epochs\": 10,       # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": batch_size,      # Batch size\n",
    "    \"seed\": 0,              # Random seed\n",
    "    \n",
    "    # Quantization\n",
    "    \"num_bits\": 4,          # Bit resolution\n",
    "    \n",
    "    # Network parameters\n",
    "    \"grad_clip\": False,     # Whether or not to clip gradients\n",
    "    \"weight_clip\": False,   # Whether or not to clip weights\n",
    "    \"batch_norm\": True,     # Whether or not to use batch normalization\n",
    "    \"dropout\": 0.07,        # Dropout rate\n",
    "    \"beta\": 1.0,           # Decay rate parameter (beta)\n",
    "    \"threshold\": 10,        # Threshold parameter (theta)\n",
    "    \"lr\": 3.0e-3,           # Initial learning rate\n",
    "    \"slope\": 5.6,           # Slope value (k)\n",
    "    \n",
    "    # Fixed params\n",
    "    \"num_steps\": 300,       # Number of timesteps to encode input for\n",
    "    \"correct_rate\": 0.8,    # Correct rate\n",
    "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
    "    \"betas\": (0.9, 0.999),  # Adam optimizer beta values\n",
    "    \"t_0\": 4690,            # Initial frequency of the cosine annealing scheduler\n",
    "    \"eta_min\": 0,           # Minimum learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        fc1_bits        = 4\n",
    "        fc2_bits        = 4\n",
    "        fc1_beta_init   = 0.1\n",
    "        fc2_beta_init   = 0.1\n",
    "        fc1_thr_init    = 1.0\n",
    "        fc2_thr_init    = 1.0\n",
    "        self.fc1_multiplier  = 10\n",
    "        self.fc2_multiplier  = 10\n",
    "\n",
    "        self.fc1_beta   = torch.nn.Parameter(torch.tensor(fc1_beta_init), requires_grad=True)\n",
    "        self.fc1        = qnn.QuantLinear(60*60, 64, bias=False, weight_bit_width=fc1_bits)\n",
    "        self.lif1       = snn.Leaky(beta=1.0, threshold=fc1_thr_init, learn_threshold=True, reset_mechanism='zero', reset_delay=False)\n",
    "\n",
    "        self.fc2_beta   = torch.nn.Parameter(torch.tensor(fc2_beta_init), requires_grad=True)\n",
    "        self.fc2        = qnn.QuantLinear(64, num_classes, bias=False, weight_bit_width=fc2_bits)\n",
    "        self.lif2       = snn.Leaky(beta=1.0, threshold=fc2_thr_init, learn_threshold=True, reset_mechanism='zero', reset_delay=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        fc_mem1   = self.lif1.init_leaky()\n",
    "        fc_mem2   = self.lif2.init_leaky()\n",
    "\n",
    "        # Record output spikes\n",
    "        spk_rec = []\n",
    "\n",
    "        scale_fc1 = self.fc1.quant_weight().scale\n",
    "        scale_fc2 = self.fc2.quant_weight().scale\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            xt = x[:, t, :, :, :]\n",
    "            xt = xt.contiguous().view(B, -1)\n",
    "\n",
    "            cur1 = self.fc1(xt)\n",
    "            fc_mem1 = NetUtils.mem_clamp(fc_mem1, scale_fc1, multiplier=self.fc1_multiplier)\n",
    "            spk1, fc_mem1 = self.lif1(cur1, fc_mem1)\n",
    "            fc_mem1 = NetUtils.beta_clamp(fc_mem1, self.fc1_beta)\n",
    "\n",
    "            cur2 = self.fc2(spk1)\n",
    "            fc_mem2 = NetUtils.mem_clamp(fc_mem2, scale_fc2, multiplier=self.fc2_multiplier)\n",
    "            spk2, fc_mem2 = self.lif2(cur2, fc_mem2)\n",
    "            fc_mem2 = NetUtils.beta_clamp(fc_mem2, self.fc2_beta)\n",
    "\n",
    "            spk_rec.append(spk2)\n",
    "\n",
    "        return torch.stack(spk_rec)\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    betas=config[\"betas\"]\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"t_0\"],\n",
    "    eta_min=config[\"eta_min\"],\n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "criterion = SF.mse_count_loss(\n",
    "    correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ea85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=\"cpu\", scheduler=None):\n",
    "    \"\"\"\n",
    "    Complete one epoch of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    lr_accum = []\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(tqdm(trainloader, leave=False, desc=\"Training\")):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        spk_rec, _ = net(data)\n",
    "        loss = criterion(spk_rec, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## Enable gradient clipping\n",
    "        if config[\"grad_clip\"]:\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "\n",
    "        ## Enable weight clipping\n",
    "        if config[\"weight_clip\"]:\n",
    "            with torch.no_grad():\n",
    "                for param in net.parameters():\n",
    "                    param.clamp_(-1, 1)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
    "        lr_accum.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return loss_accum, lr_accum\n",
    "\n",
    "def test(config, net, testloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculate accuracy on full test set.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = net(images)\n",
    "            accuracy = SF.accuracy_rate(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "loss_list = []\n",
    "lr_list = []\n",
    "\n",
    "## Load model instead of training\n",
    "load_model = True\n",
    "if load_model:\n",
    "    checkpoint = torch.load('../models/fenrir_dvsgesture_FenrirFC_best.pth', map_location=device, weights_only=False)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    net.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n",
    "iter_test = iter(testloader)\n",
    "data_it, targets_it = next(iter_test)\n",
    "\n",
    "dataset = testloader.dataset\n",
    "num_samples = len(dataset)\n",
    "ran_idx = torch.randint(0, num_samples, (3,))\n",
    "\n",
    "for i, idx in enumerate(ran_idx):\n",
    "    # Get some data\n",
    "    spike_data, target = dataset[idx]\n",
    "    spike_data = spike_data.to(device)\n",
    "    spike_data = spike_data.unsqueeze(0)\n",
    "\n",
    "    print(spike_data.shape)\n",
    "\n",
    "    # Forward pass\n",
    "    print(spike_data.shape)\n",
    "    spk_rec = net(spike_data)\n",
    "\n",
    "    # BesvÃ¦rgelse (just summing the spikes)\n",
    "    pred = torch.argmax(spk_rec.sum(dim=0).squeeze()).item()\n",
    "\n",
    "    # Plot\n",
    "    splt.raster(spk_rec[:, 0].view(180, -1), ax[i], s=25, c=\"black\")\n",
    "    ax[i].set_yticks(np.arange(0, 11, 1))\n",
    "    ax[i].set_title(f\"Prediction: {pred}, Target: {target}\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "spike_sums = spk_rec.sum(dim=0).squeeze()  # shape: [10]\n",
    "spike_avg = spike_sums.sum(dim=0)/11\n",
    "\n",
    "print(f\"Spike counts per neuron: {spike_sums.tolist()}\")\n",
    "print(f\"Average spikes per neuron: {spike_avg.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed144b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fenrir.export_weights(net.fc1, 32, 60*60, '../../src/design_sources/data/FenrirFC_fc1.data')\n",
    "quant_scale = net.fc1.quant_weight().scale\n",
    "beta        = net.fc1_beta/net.fc1.quant_weight().scale\n",
    "thr         = fenrir.get_threshold(net.fc1, net.lif1)\n",
    "print(f\"Quant Scale: {quant_scale}\")\n",
    "print(f\"Beta: {beta}\")\n",
    "print(f\"Threshold: {thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fenrir.export_weights(net.fc2, 44, 64, '../../src/design_sources/data/FenrirFC_fc2.data')\n",
    "quant_scale = net.fc2.quant_weight().scale\n",
    "beta        = net.fc2_beta/net.fc2.quant_weight().scale\n",
    "thr         = fenrir.get_threshold(net.fc2, net.lif2)\n",
    "print(f\"Quant Scale: {quant_scale}\")\n",
    "print(f\"Beta: {beta}\")\n",
    "print(f\"Threshold: {thr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d1412",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "- 1: hand_clapping\n",
    "- 2: right_hand_wave\n",
    "- 3: left_hand_wave\n",
    "- 4: right_hand_clockwise\n",
    "- 5: right_hand_counter_clockwise\n",
    "- 6: left_hand_clockwise\n",
    "- 7: left_hand_counter_clockwise\n",
    "- 8: forearm_roll_forward\n",
    "- 8: forearm_roll_backwards\n",
    "- 9: drums\n",
    "- 10: guitar\n",
    "- 11: random_other_gesturesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eea801",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select a datapoint from the dataset\n",
    "dataset = testloader.dataset\n",
    "spike_data, target_tmp = dataset[1]\n",
    "\n",
    "## Spike encode it\n",
    "spike_data = spike_data.unsqueeze(0)\n",
    "spike_data = spike_data.to(device)\n",
    "\n",
    "## Forward pass\n",
    "spk_rec1 = net(spike_data)\n",
    "pred = torch.argmax(spk_rec1.sum(dim=0).squeeze()).item()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "splt.raster(spike_data[0, :, :].view(300, -1), ax[0], s=1, c=\"black\")\n",
    "splt.raster(spk_rec1[:, 0].view(180, -1), ax[1], s=25, c=\"black\")\n",
    "ax[0].set_title(f\"Prediction: {pred}, Target: {target_tmp}\")\n",
    "\n",
    "## Detach as we will use these for comparison with the testbench\n",
    "spk_rec1 = spk_rec1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f29d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    nrn_sum = spk_rec1[:, 0, i].sum()\n",
    "    print(f\"nrn{i}_sum = {nrn_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04865588",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = spike_data[0, :, 0, :, :].reshape(spike_data.shape[1], -1)\n",
    "tsteps = 180\n",
    "events = []\n",
    "tstep_event_idx = []\n",
    "\n",
    "for t in range(tsteps):\n",
    "    t_data = exp_data[t, :]\n",
    "    non_zero_indices = (t_data != 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    for idx in non_zero_indices.tolist():\n",
    "        events.append(idx)\n",
    "\n",
    "    events.append(0b1000000000000)  # marker\n",
    "\n",
    "    tstep_event_idx.append(len(events))\n",
    "\n",
    "# Create a C header file with the events as an array\n",
    "header_file = 'gesture_data_target_1.h'\n",
    "\n",
    "with open(header_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('#ifndef NMNIST_FPGA_DATA_H\\n')\n",
    "    f.write('#define NMNIST_FPGA_DATA_H\\n\\n')\n",
    "\n",
    "    # Write array size\n",
    "    f.write(f'#define NMNIST_EVENTS_SIZE {len(events)}\\n\\n')\n",
    "\n",
    "    # Write the array data\n",
    "    f.write('const unsigned int nmnist_events[NMNIST_EVENTS_SIZE] = {\\n')\n",
    "\n",
    "    # Write values as hex or binary, here hex is easier for C\n",
    "    for i, val in enumerate(events):\n",
    "        # Write comma except last element\n",
    "        comma = ',' if i < len(events) - 1 else ''\n",
    "        f.write(f'    0x{val:X}{comma}\\n')\n",
    "\n",
    "    f.write('};\\n\\n')\n",
    "    f.write('#endif // NMNIST_FPGA_DATA_H\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ca4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = spike_data[0, :, 0, :, :].reshape(spike_data.shape[1], -1)\n",
    "tsteps = 180\n",
    "events = []\n",
    "tstep_event_idx = []\n",
    "\n",
    "for t in range(tsteps):\n",
    "    t_data = exp_data[t, :]\n",
    "    non_zero_indices = (t_data != 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    for idx in non_zero_indices.tolist():\n",
    "        events.append(idx)\n",
    "\n",
    "    # Add special 13-bit marker (MSB = 1)\n",
    "    events.append(0b1000000000000)\n",
    "    tstep_event_idx.append(len(events))\n",
    "\n",
    "# Format with 13 bits: MSB + 12-bit index\n",
    "binary_events = []\n",
    "for idx in events:\n",
    "    if idx == 0b1000000000000:\n",
    "        binary_events.append('1000000000000')  # reserved marker\n",
    "    elif idx < 4096:\n",
    "        binary_events.append('0' + format(idx, '012b'))  # MSB 0 + 12-bit index\n",
    "    else:\n",
    "        raise ValueError(f\"Index {idx} exceeds 12-bit range\")\n",
    "\n",
    "# Write to file\n",
    "out_file = 'gesture_data.txt'\n",
    "with open(out_file, 'w', encoding='utf-8') as f:\n",
    "    for b in binary_events:\n",
    "        f.write(b + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvs-fpga-_hAg3Ylq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
