{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized SNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch.functional import quant\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "# Quantization\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Tonic\n",
    "from tonic import DiskCachedDataset\n",
    "from tonic import MemoryCachedDataset\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pyfenrir as fenrir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_path  = './data/mnist'\n",
    "num_class  = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Make a sanity check plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for i in range(3):\n",
    "    image, label = mnist_train[i]\n",
    "    axs[i].imshow(image.squeeze(0).numpy(), cmap=\"gray\")\n",
    "    axs[i].set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataloader and Spike Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "cached_mnist_train = MemoryCachedDataset(mnist_train)\n",
    "cached_mnist_test = MemoryCachedDataset(mnist_test)\n",
    "\n",
    "train_loader = DataLoader(cached_mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(cached_mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- The rest in this cell is just to show how to encode data into spikes ---\n",
    "\n",
    "# Fetch a batch of data\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "# Encode it into spikes\n",
    "#   - linear, normalize, clip are IMPORTANT for the spikegen.latency function\n",
    "#     details here: https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_1.html\n",
    "spike_data = spikegen.latency(data_it, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "\n",
    "# Plot to keep me sane\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(12, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "splt.raster(spike_data[:, 0].view(100, -1), ax, s=25, c=\"black\")\n",
    "plt.title(\"Input Layer\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Neuron Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_epochs\": 2,       # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": 128,      # Batch size\n",
    "    \"seed\": 0,              # Random seed\n",
    "    \n",
    "    # Quantization\n",
    "    \"num_bits\": 4,          # Bit resolution\n",
    "    \n",
    "    # Network parameters\n",
    "    \"grad_clip\": False,     # Whether or not to clip gradients\n",
    "    \"weight_clip\": False,   # Whether or not to clip weights\n",
    "    \"batch_norm\": True,     # Whether or not to use batch normalization\n",
    "    \"dropout\": 0.07,        # Dropout rate\n",
    "    \"beta\": 1.0,           # Decay rate parameter (beta)\n",
    "    \"threshold\": 10,        # Threshold parameter (theta)\n",
    "    \"lr\": 3.0e-3,           # Initial learning rate\n",
    "    \"slope\": 5.6,           # Slope value (k)\n",
    "    \n",
    "    # Fixed params\n",
    "    \"num_steps\": 100,       # Number of timesteps to encode input for\n",
    "    \"correct_rate\": 0.8,    # Correct rate\n",
    "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
    "    \"betas\": (0.9, 0.999),  # Adam optimizer beta values\n",
    "    \"t_0\": 4690,            # Initial frequency of the cosine annealing scheduler\n",
    "    \"eta_min\": 0,           # Minimum learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# First train with learnable beta and threshold\n",
    "# Round the values to integers and train again with those.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_bits   = config[\"num_bits\"]\n",
    "        self.thr        = config[\"threshold\"]\n",
    "        self.slope      = config[\"slope\"]\n",
    "        self.num_steps  = config[\"num_steps\"]\n",
    "        self.batch_norm = config[\"batch_norm\"]\n",
    "        \n",
    "        self.beta = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "        # Initialize Layers\n",
    "        self.fc1        = qnn.QuantLinear(32*32, 10, bias=False, weight_bit_width=self.num_bits)\n",
    "        self.lif1       = snn.Leaky(beta=1.0, threshold=1.0, learn_threshold=True, reset_mechanism='zero', reset_delay=False)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        for step in range(self.num_steps):\n",
    "            cur = self.fc1(x[step].view(x.shape[1], -1))\n",
    "\n",
    "            spk1, mem1 = self.lif1(cur, mem1)\n",
    "\n",
    "            mem1 = torch.where(\n",
    "                mem1 > 0,\n",
    "                torch.clamp(mem1 - self.beta, min=0.0),\n",
    "                mem1\n",
    "            )\n",
    "\n",
    "            mem1 = torch.where(\n",
    "                mem1 < 0,\n",
    "                torch.clamp(mem1 + self.beta, max=0.0),\n",
    "                mem1\n",
    "            )\n",
    "\n",
    "            spk_rec.append(spk1)\n",
    "            mem_rec.append(mem1)\n",
    "        \n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "net = Net(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define Optimizer, LR Scheduler and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    betas=config[\"betas\"]\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"t_0\"],\n",
    "    eta_min=config[\"eta_min\"],\n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "criterion = SF.mse_count_loss(\n",
    "    correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=\"cpu\", scheduler=None):\n",
    "    \"\"\"\n",
    "    Complete one epoch of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    lr_accum = []\n",
    "    for data, labels in trainloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        # Encode data into spikes\n",
    "        data = spikegen.latency(data, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "        spk_rec, mem_rec = net(data)\n",
    "        loss = criterion(spk_rec, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## Enable gradient clipping\n",
    "        if config[\"grad_clip\"]:\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "\n",
    "        ## Enable weight clipping\n",
    "        if config[\"weight_clip\"]:\n",
    "            with torch.no_grad():\n",
    "                for param in net.parameters():\n",
    "                    param.clamp_(-1, 1)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
    "        lr_accum.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return loss_accum, lr_accum\n",
    "\n",
    "def test(config, net, testloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculate accuracy on full test set.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = spikegen.latency(images, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "            outputs, _ = net(images)\n",
    "            accuracy = SF.accuracy_rate(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "loss_list = []\n",
    "lr_list = []\n",
    "\n",
    "## Load model instead of training\n",
    "load_model = True\n",
    "if load_model:\n",
    "    net.load_state_dict(torch.load('models/mnist.pth'))\n",
    "else:\n",
    "    print(f\"=======Training Network=======\")\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        loss, lr  = train(config, net, train_loader, criterion, optimizer, device, scheduler)\n",
    "        loss_list = loss_list + loss\n",
    "        lr_list   = lr_list + lr\n",
    "        # Test\n",
    "        test_accuracy = test(config, net, test_loader, device)\n",
    "        print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy}\")\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(loss_list, color='tab:orange')\n",
    "    ax2.plot(lr_list, color='tab:blue')\n",
    "    ax1.set_xlabel('Iterationb')\n",
    "    ax1.set_ylabel('Loss', color='tab:orange')\n",
    "    ax2.set_ylabel('Learning Rate', color='tab:blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    dir = \"./models\"\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    torch.save(net.state_dict(), f\"{dir}/mnist.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Network Results and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n",
    "iter_test = iter(test_loader)\n",
    "data_it, targets_it = next(iter_test)\n",
    "\n",
    "dataset = test_loader.dataset\n",
    "num_samples = len(dataset)\n",
    "ran_idx = torch.randint(0, num_samples, (3,))\n",
    "\n",
    "for i, idx in enumerate(ran_idx):\n",
    "    # Get some data\n",
    "    data_tmp, target_tmp = dataset[idx]\n",
    "\n",
    "    # Spike encode it\n",
    "    spike_data = spikegen.latency(data_tmp, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "    spike_data = spike_data.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    spk_rec, mem_rec = net(spike_data)\n",
    "\n",
    "    # BesvÃ¦rgelse (just summing the spikes)\n",
    "    pred = torch.argmax(spk_rec.sum(dim=0).squeeze()).item()\n",
    "\n",
    "    # Plot\n",
    "    splt.raster(spk_rec[:, 0].view(100, -1), ax[i], s=25, c=\"black\")\n",
    "    ax[i].set_yticks(np.arange(0, 10, 1))\n",
    "    ax[i].set_title(f\"Prediction: {pred}, Target: {target_tmp}\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "spike_sums = spk_rec.sum(dim=0).squeeze()  # shape: [10]\n",
    "spike_avg = spike_sums.sum(dim=0)/10\n",
    "\n",
    "print(f\"Spike counts per neuron: {spike_sums.tolist()}\")\n",
    "print(f\"Average spikes per neuron: {spike_avg.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Extract Quantized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenrir.export_weights(net.fc1, 40, 1024, 'weights.txt')\n",
    "quant_scale = net.fc1.quant_weight().scale\n",
    "beta        = net.beta/net.fc1.quant_weight().scale\n",
    "thr         = fenrir.get_threshold(net.fc1, net.lif1)\n",
    "print(f\"Quant Scale: {quant_scale}\")\n",
    "print(f\"Threshold: {thr}\")\n",
    "print(f\"Beta: {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Spike Encoded Number for Testbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select a datapoint from the dataset\n",
    "dataset = test_loader.dataset\n",
    "data_tmp, target_tmp = dataset[6]\n",
    "\n",
    "## Spike encode it\n",
    "spike_data = spikegen.latency(data_tmp, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "spike_data = spike_data.to(device)\n",
    "\n",
    "## Forward pass\n",
    "spk_rec, mem_rec = net(spike_data)\n",
    "pred = torch.argmax(spk_rec.sum(dim=0).squeeze()).item()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "splt.raster(spike_data[:, 0].view(100, -1), ax[0], s=25, c=\"black\")\n",
    "splt.raster(spk_rec[:, 0].view(100, -1), ax[1], s=25, c=\"black\")\n",
    "ax[0].set_title(f\"Prediction: {pred}, Target: {target_tmp}\")\n",
    "\n",
    "## Detach as we will use these for comparison with the testbench\n",
    "spk_rec = spk_rec.cpu().detach().numpy()\n",
    "mem_rec = mem_rec.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_signed_int(bstr, bits=12):\n",
    "    \"\"\"Convert binary string to signed integer.\"\"\"\n",
    "    val = int(bstr, 2)\n",
    "    if val >= 2**(bits - 1):\n",
    "        val -= 2**bits\n",
    "    return val\n",
    "\n",
    "names = [\n",
    "    't',\n",
    "    'nrn_addr',\n",
    "    'val'\n",
    "]\n",
    "df_mem = pd.read_csv('../vivado/fenrir/fenrir.sim/sim_1/behav/xsim/mem_rec.csv', names=names)\n",
    "\n",
    "names = [\n",
    "    't',\n",
    "    'nrn_addr',\n",
    "]\n",
    "df_spk = pd.read_csv('../vivado/fenrir/fenrir.sim/sim_1/behav/xsim/spk_rec.csv', names=names)\n",
    "df_spk['nrn_addr'] = df_spk['nrn_addr'].apply(lambda b: int(str(b), 2))\n",
    "\n",
    "nrn_mem = {\n",
    "    '0': [],\n",
    "    '1': [],\n",
    "    '2': [],\n",
    "    '3': [],\n",
    "    '4': [],\n",
    "    '5': [],\n",
    "    '6': [],\n",
    "    '7': [],\n",
    "    '8': [],\n",
    "    '9': [],\n",
    "}\n",
    "\n",
    "for t in range(0, 100-1):\n",
    "    subset = df_mem[df_mem['t'] == t][['nrn_addr', 'val']].head(4)\n",
    "    addr_list = subset['nrn_addr'].tolist()\n",
    "    val_list = subset['val'].tolist()\n",
    "\n",
    "    nrn_mem['2'].append(bin_to_signed_int(val_list[0][0:12], bits=12))\n",
    "    nrn_mem['1'].append(bin_to_signed_int(val_list[0][12:24], bits=12))\n",
    "    nrn_mem['0'].append(bin_to_signed_int(val_list[0][24:36], bits=12))\n",
    "    nrn_mem['5'].append(bin_to_signed_int(val_list[1][0:12], bits=12))\n",
    "    nrn_mem['4'].append(bin_to_signed_int(val_list[1][12:24], bits=12))\n",
    "    nrn_mem['3'].append(bin_to_signed_int(val_list[1][24:36], bits=12))\n",
    "    nrn_mem['8'].append(bin_to_signed_int(val_list[2][0:12], bits=12))\n",
    "    nrn_mem['7'].append(bin_to_signed_int(val_list[2][12:24], bits=12))\n",
    "    nrn_mem['6'].append(bin_to_signed_int(val_list[2][24:36], bits=12))\n",
    "    nrn_mem['9'].append(bin_to_signed_int(val_list[3][24:36], bits=12))\n",
    "\n",
    "x = np.arange(0, 99, 1)\n",
    "spk_0 = np.zeros_like(x)\n",
    "spk_1 = np.zeros_like(x)\n",
    "spk_2 = np.zeros_like(x)\n",
    "spk_3 = np.zeros_like(x)\n",
    "spk_4 = np.zeros_like(x)\n",
    "spk_5 = np.zeros_like(x)\n",
    "spk_6 = np.zeros_like(x)\n",
    "spk_7 = np.zeros_like(x)\n",
    "spk_8 = np.zeros_like(x)\n",
    "spk_9 = np.zeros_like(x)\n",
    "\n",
    "for t, nrn in zip(df_spk['t'], df_spk['nrn_addr']):\n",
    "    if nrn == 0:\n",
    "        spk_0[t] = 1\n",
    "    elif nrn == 1:\n",
    "        spk_1[t] = 1\n",
    "    elif nrn == 2:\n",
    "        spk_2[t] = 1\n",
    "    elif nrn == 3:\n",
    "        spk_3[t] = 1\n",
    "    elif nrn == 4:\n",
    "        spk_4[t] = 1\n",
    "    elif nrn == 5:\n",
    "        spk_5[t] = 1\n",
    "    elif nrn == 6:\n",
    "        spk_6[t] = 1\n",
    "    elif nrn == 7:\n",
    "        spk_7[t] = 1\n",
    "    elif nrn == 8:\n",
    "        spk_8[t] = 1\n",
    "    elif nrn == 9:\n",
    "        spk_9[t] = 1\n",
    "\n",
    "spks = [spk_0, spk_1, spk_2, spk_3, spk_4, spk_5, spk_6, spk_7, spk_8, spk_9]\n",
    "\n",
    "nrn = 0\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 7))\n",
    "\n",
    "ax[0].plot(x, nrn_mem[f\"{nrn}\"], color='red', linewidth=2, alpha=0.5, label=\"TB\")\n",
    "\n",
    "scale = net.fc1.quant_weight().scale.cpu().detach().numpy()\n",
    "thr = fenrir.get_threshold(net.fc1, net.lif1)*100\n",
    "ax[0].plot(x, np.full_like(x, thr), linestyle='--', color='black', label=\"Thr\")\n",
    "ax[0].plot(x, np.full_like(x, 0), linestyle=':', color='black', alpha=0.5)\n",
    "ax[0].plot(mem_rec[:, 0, nrn]*100/scale, color='blue', linewidth=2, alpha=0.5, label=\"snntorch\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(x, spks[nrn], color='red', linewidth=2, alpha=0.5)\n",
    "ax[1].plot(x, spk_rec[:99, 0, nrn], color='blue', linewidth=2, alpha=0.5)\n",
    "\n",
    "x_start = 4\n",
    "x_end   = 100\n",
    "ax[0].set_xlim(x_start, x_end)\n",
    "ax[1].set_xlim(x_start, x_end)\n",
    "ax[0].set_xticks(np.arange(x_start, x_end + 1, 10))\n",
    "ax[1].set_xticks(np.arange(x_start, x_end + 1, 10))\n",
    "\n",
    "print(nrn_mem['0'][19])\n",
    "print(mem_rec[19, 0, 0]/scale*100)\n",
    "ax[0].set_title(\"mem_rec\")\n",
    "ax[1].set_title(\"spk_rec\")\n",
    "fig.suptitle(f\"Neuron {nrn}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_mem[df_mem['t'] == 1][['nrn_addr', 'val']].tail(4)\n",
    "addr_list = subset['nrn_addr'].tolist()\n",
    "val_list = subset['val'].tolist()\n",
    "\n",
    "nrn_mem = {\n",
    "    '0': [],\n",
    "    '1': [],\n",
    "    '2': [],\n",
    "    '3': [],\n",
    "    '4': [],\n",
    "    '5': [],\n",
    "    '6': [],\n",
    "    '7': [],\n",
    "    '8': [],\n",
    "    '9': [],\n",
    "}\n",
    "\n",
    "nrn_mem['2'].append(bin_to_signed_int(val_list[0][0:12], bits=12))\n",
    "nrn_mem['1'].append(bin_to_signed_int(val_list[0][12:24], bits=12))\n",
    "nrn_mem['0'].append(bin_to_signed_int(val_list[0][24:36], bits=12))\n",
    "nrn_mem['5'].append(bin_to_signed_int(val_list[1][0:12], bits=12))\n",
    "nrn_mem['4'].append(bin_to_signed_int(val_list[1][12:24], bits=12))\n",
    "nrn_mem['3'].append(bin_to_signed_int(val_list[1][24:36], bits=12))\n",
    "nrn_mem['8'].append(bin_to_signed_int(val_list[2][0:12], bits=12))\n",
    "nrn_mem['7'].append(bin_to_signed_int(val_list[2][12:24], bits=12))\n",
    "nrn_mem['6'].append(bin_to_signed_int(val_list[2][24:36], bits=12))\n",
    "nrn_mem['9'].append(bin_to_signed_int(val_list[3][24:36], bits=12))\n",
    "\n",
    "for nrn_addr, values in nrn_mem.items():\n",
    "    print(f\"Neuron {nrn_addr}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 98, 1)\n",
    "spk_0 = np.zeros_like(x)\n",
    "\n",
    "spk_0[97]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 0\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(mem_rec[:, 0, neuron])\n",
    "ax[1].plot(spk_rec[:, 0, neuron])\n",
    "\n",
    "thr = net.lif1.threshold.cpu().detach().item()\n",
    "x   = np.arange(0, 100, 1)\n",
    "ax[0].plot(x, np.full_like(x, thr, dtype=np.float32))\n",
    "ax[0].plot(x, np.full_like(x, 0), linestyle='--')\n",
    "\n",
    "spike_t = spk_rec[:, 0, neuron]\n",
    "for t, s in enumerate(spike_t):\n",
    "    if s > 0:\n",
    "        ax[0].axvline(x=t, color='black', linestyle=':', alpha=0.5)\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "\n",
    "spk_events = []\n",
    "for nrn in range(0, 10):\n",
    "    spk_events.append(spk_rec[t, 0, nrn])\n",
    "\n",
    "spk_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract input events for specific time index\n",
    "t_events = []\n",
    "\n",
    "tstep_data = spike_data[:, 0, :, :]\n",
    "tstep_data = tstep_data.view(100, -1).cpu().detach().numpy()\n",
    "\n",
    "for tstep in range(0, 100):\n",
    "    temp = tstep_data[tstep, :]\n",
    "    non_zero_indices = np.nonzero(temp)[0]\n",
    "    t_events.append(non_zero_indices.tolist())\n",
    "\n",
    "print(t_events[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the weights from certain input positions to output neuron\n",
    "weights= net.fc1.quant_weight()\n",
    "scale = net.fc1.quant_weight().scale.cpu().detach().numpy()\n",
    "\n",
    "# in_nrns = [457, 492, 499, 531]\n",
    "in_nrns = [236, 297, 523, 525, 783]\n",
    "weights = weights[0][0, in_nrns].cpu().detach().numpy()\n",
    "\n",
    "q_weights = weights/scale\n",
    "q_weights_sum = q_weights.sum()\n",
    "\n",
    "print(q_weights)\n",
    "print(q_weights_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export for FENRIR\n",
    "fenrir.export_spike_data(spike_data, 'test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data    = spike_data[:, 0, :, :]\n",
    "exp_data    = exp_data.view(exp_data.shape[0], -1)\n",
    "tsteps      = spike_data.shape[0]\n",
    "events      = []\n",
    "tstep_event_idx = []\n",
    "\n",
    "for t in range(0, tsteps):\n",
    "    t_data = exp_data[t, :]\n",
    "    non_zero_indices = (t_data != 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    #events.append(0b1000000000000)\n",
    "\n",
    "    for idx in non_zero_indices.tolist():\n",
    "        events.append(idx)\n",
    "\n",
    "    events.append(0b1000000000000)\n",
    "\n",
    "    tstep_event_idx.append(len(events))\n",
    "\n",
    "binary_events = [format(idx, '010b') for idx in events]\n",
    "\n",
    "out_file = 'test_data.txt'\n",
    "with open(out_file, 'w') as f:\n",
    "    for b in binary_events:\n",
    "        if not b == '1000000000000':\n",
    "            f.write(\"000\" + b + '\\n')\n",
    "        else:\n",
    "            f.write(b + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raster plot of snntorch versus fenrir\n",
    "dataset = test_loader.dataset\n",
    "data_tmp, target_tmp = dataset[6]\n",
    "\n",
    "spike_data = spikegen.latency(data_tmp, num_steps=100, tau=5, threshold=0.01, linear=True, normalize=True, clip=True)\n",
    "spike_data = spike_data.to(device)\n",
    "spk_rec, mem_rec = net(spike_data)\n",
    "pred = torch.argmax(spk_rec.sum(dim=0).squeeze()).item()\n",
    "\n",
    "nrn_arr = np.stack([nrn_0, nrn_1, nrn_2, nrn_3, nrn_4, nrn_5, nrn_6, nrn_7, nrn_8, nrn_9], axis=1)\n",
    "pred_nrn = np.argmax(nrn_arr.sum(axis=0))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 7))\n",
    "splt.raster(spk_rec[:, 0].view(100, -1), ax[0], s=25, c=\"black\")\n",
    "splt.raster(torch.from_numpy(nrn_arr).view(100, -1), ax[1], s=25, c=\"black\")\n",
    "ax[0].set_title(f\"snntorch pred: {pred}, Target: {target_tmp}\")\n",
    "ax[1].set_title(f\"fenrir pred: {pred_nrn}, Target: {target_tmp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
