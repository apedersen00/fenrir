{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4620d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "import tonic.transforms as tonic_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc8029cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/nmnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1515dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 60000\n",
      "Number of classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "Sample shape: (array([(10, 30,    937, 1), (33, 20,   1030, 1), (12, 27,   1052, 1), ...,\n",
      "       ( 7, 15, 302706, 1), (26, 11, 303852, 1), (11, 17, 305341, 1)],\n",
      "      dtype=[('x', '<i4'), ('y', '<i4'), ('t', '<i4'), ('p', '<i4')]), 0)\n"
     ]
    }
   ],
   "source": [
    "# (Down)load the dataset and print some information\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(\n",
    "    save_to='./data/nmnist',\n",
    "    train=True\n",
    ")\n",
    "\n",
    "print(f\"Number of samples: {len(dataset)}\"\n",
    "      f\"\\nNumber of classes: {dataset.classes}\"\n",
    "      f\"\\nSample shape: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "451a2cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAACBCAYAAACma0xyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZTUlEQVR4nO3dX4yc1X3G8UOAeL3Y69n1rjE2tpfaGNYIS6EE4wo2KlEtEVUFoqooIq1aKarSiyREKFeVckfvUB3lAqmK1ErFF1RqAakKki+otInq8Cd2ZAtPbGxhbGwMs16Pdzfr2djgXqQ+5zln57z77vx958z3c3Vm5n1n3pnZ3ffs+5zzO7fcuHHjhgEAAEBP+1K3DwAAAADNo1MHAACQADp1AAAACaBTBwAAkAA6dQAAAAmgUwcAAJAAOnUAAAAJoFMHAACQgNvybnjPT15q53GgTT78wQsN78t33pv4zvtPUb7zsfumvduVE6Mte+5GjqEbr98pRfnO0Tl5vnOu1AEAACSATh0AAEACcsevAJozsHXOtmtn13bxSID2KELcWYRjALqFK3UAAAAJoFMHAACQAOLXbhlddO3pVd07DnRMv0WuN4avebdvuXx7l44kHfqZ8nmiYZx/ksWVOgAAgATQqQMAAEgAnToAAIAEFGNMXT/m++18n/p5Ai3QyPg4xnzll3esHJ8pWqIXz7Phea0X30MHcKUOAAAgAXTqAAAAElCM+JXLqK3F51lXnhUddJus7foNsV/rUZ6kt4zdN+3dZuWKDuO8lgtX6gAAABJApw4AACABxYhfgTbJE7mqvHHrSp8XvW/VjPsfeHHkiy4eCbqBuLUP9eCMW67UAQAAJIBOHQAAQAKSiF91VtLEyKe2XZ6509uOy+f9p13RaCPPS2TbXUOn6/8Puzgc3I5Eq62OXBuZ8dqOGbP697NX/0aWyrfUvb86caPDR5Ithc+63WLf5WDF//27MCnbSUw6tt6vYHBT5VLwNzdPlNoDcWuIK3UAAAAJoFMHAACQADp1AAAACeiZMXVhNW8dO/evW39h2+8sysLj/pA68/LIE7at4+28rD0rQ9fpzT2YtaO7GEfXGVp6ZN1pNw6ndLxq21/6dMa2v7hzxNu/umvItmsjUsZExt51q6RJO1ae6MWxXTteXYg+Nr37jrr3h+cQpZ/B8/vetO39B59s4OjievGzbodw3JyOlxt672Pbvn7OtRee2eM/iZyPJ+89Vfd1Hlp71rYPmK/6D8rYO+976fHzPFfqAAAAEkCnDgAAIAHFjl/lMqjGrcb4kWteXkx7p4tpX/7UxbLmXtec+mDHil8D7dNsSRBKirRHtxem17jVGGM2vv1721597ONwc2OMMdc/uWjbXwri15H/OWPbGs0ubhi07cs7v+ztM7udFSYaFcaisYjSi+x+ddR/8NHdthkrY5I3+jzwkYvpNIrV+1fyfPiDgYr7Pd345tnodhq53rblbtteGPN/zwfKq217avoB94DGp3I+f27bu97+ewZdZPv2Nneu1++5Eh5cD8SxXKkDAABIAJ06AACABBQ6fn3+obdsWy+VGmPMTy+7y6WH57ba9j/c+ZaJ+enlbbb9veGPbPuRyOxZnTljjDH7Dz9h0D0amTYSpRK5tke3I1ed4WqMH7lefdDFN6s+kxmTGr/KTNgs+ryrj/mPDfzpuG1/FkyyQ7asGFMj19F/OeQekLjVGGNOPTtolpN3NYdrr4/Z9v4JN/tVo1hjjDHudNLQLNl+WF1CI9fxf8+IXDfLEAhpn3lijW3XxsIhDi5m19epGReRaixbvi8ohyHfn/YHtK/x/fK3vF28OLagUSxX6gAAABJApw4AACABhYtfYwUi317wZ6JqFKqFB3Um69ShB7x9vFkxEu2Gs5puCmfL6Ot4s22yNFvIMO/+PV4wMY9Y5Kr3ZyF+TYfOcA1p5Hp19DZpu6LCqzc8bNuzo/E/g6unr7sbGbNfh0+649nwrns+otj6sqLHWOQ6/fd7bftvf/Bzbx+NP3X/259eMn9xyTYhfe5/+8k3XLv8DW87fe5GotRUI1c1Uv687v2zD2/2buvM1tqo+26WRq71edtFzoXh573/hPuZORDpd4R9AC1gXNQolit1AAAACaBTBwAAkAA6dQAAAAkoxJg6HY+gK0foWLe5X27w9hmQto5v07ESO47+zttHF3reb5YvTxIuAKzHlnsMRbNZe979C5Tpt0o4Vu6RzTIlfrPJ5Z3zW+ven7ckCqtQtE6zK08MnXb/g+pYudqI/7+pru6g++i4t4t7/DFxMbURt93isLt/ccQf6zMwU/9Pqb5+P6w6kbU6RN6/mbHSJTrWLSwhEhsjp2Oi9HwSrjoR21+3C7fR0id6bN64qz4YNxfaNOU+s6H3ztu2jqMLV4eoPl5zN2LnMh0rlyWyv5Y9McYfh6f9i9rEVds+POKfP8KVrW6qFOj8y5U6AACABNCpAwAASEAh4teYyiUpWxE8NjDtLvHe/ZarFH/beVcdXhcGNsaYjeddqYPBirsUPDNxq23rJdnwkmpl1B2PljfR+1OMQYviqfVHbPvpO+Zt++/OPm7bYdyqkanGAhcmWYWiVfLGqs2uPKHx5+x21/7m1w552z04eM62/+Oii8LKR6SEvIlHobpahdIyKmF8e0WOZ9Xl6FMnLytuzB1FSuSaZ6WIVtMyJmNStqRi4qVXNA7WmDmMbMPYN3UaucbOs8YYM7ZehtpI2/uZCc+tkTg2jFlXaqC82rbD0mWTe9+vv1NWNNzhPgFX6gAAABJApw4AACAB3Ylfg0uVOkMpVhl8sOJXptZZNXlpHDvkPSJTKcuuOfjJVW+rj2Vx4fJ6tziwXjou0iyYXhFbEcKb7Rp44NBztp03Ir0wGa8iH7PmrPu/Z35r+rMXG6GxarMzXLPojNNvfu1t29a4NfRP46/Z9m82uuEXGsuGzlVL7jV/7TLfrBmz/mxY9zOz7nT9+40p5mxY/Zvbqahwxw9/5d8h8evz+960bZ29mjfWjMWiWbznyoiMozNj73PNJStaFHE2bJOrEYVx5+AnbmiMVpzQyDX8LnRW6dQH/upRdY8zlOO4l6xOIc9Xk69F49fwvekqVXl/njqNK3UAAAAJoFMHAACQgK7Er95MF2PMnkE3k1QviQ5WXNyRFbfqDJvMWFYu61+PbxWlixPPGFes8JG/OGbb3kxYY/IVUuzzyFbj05f+/JXodm9c+optj7ziovALk/Hn1hmvs+O31t1m6Ez9Raf/sM/yz0Us6zQbuWp8Gz6fPlae3WjbYfz6N0MuFvnRxb11t/Ni2ZqLZY0x5tjQFts+aO637fkP10WPOzZjNiyMXHRFmJ0Zm/HqzYRs4DizZt+28n3HCi7nlVXAuS2aPP9oJYpQbVRj8vh25Zk7o49ZOY9zScya4/manTHrDcG61N2KCb31FwcAAAB10akDAABIAJ06AACABBRiRYnvl79l2zqdWKdGX9884u2j5UW8TF8XDX5yi+7ijZvQ8VFKFxpeGLvDe2yw4rJ6HV83NeGmYIfjBaMlTvp8HJ2WMYmVLnnhv7/t3b7vZ65U/+w+HR/nvpfwex18zZW+uPDPj9q2lipZe7Jq2+f3rff298fLudfUcXjzW/OVStH3XNSVKvKWJMmzXdb4uJi1pQXv9pxx46u8/cdd89iC/3v+I7mtY+9issqbbClVbfvcPe7+rPF1Wt5k1eX4/806Ds8vidJnZKyzMf6YMi1jUjR6nNfKY7YdK28S7hMbK9f2MXQtNnr0d97thbvcOVzHt3nj1oLPpdv0OMNyOT73HmIrTIU6XeaMK3UAAAAJoFMHAACQgK7Er1o92hi/gvQmiTX1Mu7S1QBildpd259ObYxOqdbn08gua2p7bdTFbxq/elHq+vqrI3RUVuXtDsobNz61/ohta+SqcasxxsztLNm2xqL6OmtP+sVq5p7ZU/c1rz92xd046JpheRONVmNR7JogPY6VOClq5KryliTJs13e51pzz5XoYwOn60cXx89scjfG/cd0RQiNSY8Pyz67629vjDFzVRf57hq/UPf1w2OeN/XjWF01YsO7/mMDM679mT+6JHlZEZeuMKTxa57ospNiJVb0vbXiOIv2vo3xo1Q9T2dtV5twKzR9d5v/y1CkmF37Dd553vh9AB0qNmVcHyaMYr0yZx2IYrlSBwAAkAA6dQAAAAnoSvwaVo/Wy5jGyKoNE/VnOIZ05kptTB/JVyVcX2dA1l8OK1Pr7YVp1x8eqMRnO3Vl5YiCzKzVuFEj0pCuFKGzUkP+ihDuu7jtlxp9XfL2WRrbL6UzXrNWl9D3MG/cewtn3OadDdvPdGasN8M0iEJXSQKvUWaW2MxUjYMPnr2/7jbhdseNi2zDmbkqtqKEzmrNWl1ilUSx/TATVmdM6qLvxvhRXFHixuVoRLruVV0Rw39vtz/tTjB531sRPwM9Fw6+ddV7LPw+b9LKEEWKW1dCK21oTBtWvWhaE/0GrtQBAAAkgE4dAABAApqPX7NmWuplw4ztYgsC516Yt0nRAokZ/ELGbv8LY/6lcm/mkj5QkIi0W17c/YZta/y6+aCLT3W2qzHxWaV6f1g8OBbbezNRI7NajfFntmrkOrn3fds+fPZBb59NUy7CzRP/9qNYlBlGpxtm3HezOCzFek39iNQYY/J84jrDdYmgaHK9fZbMiv161TbLR7bZtl+A2f+dH5ipP4N/sQ9mwsYiulARZ37Wo8d2bbf7CQwL81552rV75b0tJ2v2qzc0Sha6z4wrY/2GLpwzdTECY/wFCKoT7nvW9xZW99D3mrsQcRPvlSt1AAAACaBTBwAAkIDm49cGLhNWLvlFWAdkFslgJdy6sxqJfP1LtBkzbvs8co155/xW2777fRdrXvjOo/U2XyI2K7URsYh3JfsMnWnqEHpO3vViYyaGLtr2+dP3BI/qOqruXo0oG3nNRvZR50qlFb/O4rD/mBYfDh/rJ2HB97HIdlrUN6tIfLfpsY0e7eKBtJE/TCmjYoAWH85ImMM+gdWFc6Y3HGvaH8yh53qtelGToRXl9X51D41jK6b9MTtX6gAAABJApw4AACABdOoAAAAS0LkVJTKycc2wZ0y+VSSKKiyJ4i3m28e0hElo5JU1tn3rA+GSHMvzypMEdLxd1nYrpeMAtbyJMX6Jk06VN8lasaPdGhmfpqtIlGc32nbWuLMiCUuiaImWNfdcse1rv843WC42XrAf7HjVL29z5cf1tyvyOLqYsHTL7ca91+dkUfv9J57s2DG1gjf2vOw/pmU/aqNyPpc+wESw6L3qxdJfet6f2OuXNAlX0Go3rtQBAAAkgE4dAABAAjoXv6qM1SVqpjcutyo/Fggigh65fNxu/3j0Ke/2I5vP1t3OXxGi+fi9lZFr7Hmf2n3Ee2xq6wO2vflg1T0w2b66Fe16n610I7JSQ166AsPs9maPpjlLIufS8vusOx3/ee61kibtXA1Bn+/5fW/adjsjytj7aeR9apwcxq9e5HqwtyLXmHDVBV1FY2ZijTzgzvthJOmV/YiVN+kQjVIHK365lvC93lSbuBp9Pu/9dGCFDK7UAQAAJIBOHQAAQALaG79GLjUOlP0FgL1Ll11ewDc3OU5vwd7g0nHpFwO23Yuzt1olFrcaY8zak1W5VbKt+a2dmS3aSHSZNds09tias+5/qEZWrkjVuWop+lhtxH1mwyd/b9urZr5s24sjvfFZrp6+7t2+vLP33sNN7VyAXleO+N6zH9n2gQ5FvnnuD+kxq/Bv/oGPvrqyAzPtjbo7Ss/n67s3W78ejVxHyvEVMmqy+pX2W55/6C3bzvyOO9Cn4UodAABAAujUAQAAJCB//NpILKqRayVf/3FSihJOmR0rf81WCmbpasyqs5gOz7lCtNde95ej1plAtVE3E8gr3pioRgrizo43V3w6fM1YtNrK2aJvXPqKd1uj5lM7d9n20Bl3WT8rWm5XweRmhDNXGyk4HKOFfAeCx3RW6NVR9+dKZ5JeCf43jUWZ+h5aefzGGDP/4bq69w9JUWE9/n7nx5L+DFH9m/nob/7StvMW621XXBmLWENXfuxmv44Zv7CyHo8+X9bQnLZHrlnn9hznfY0kjTFm4S43vOrut+Zt+9Sz7vc8HKaks1+94Uy6UZN9gLAPojHr0Hv1hwddfHKLd3vtY5/Z9ncjfYBMzH4FAABAHnTqAAAAEkCnDgAAIAH5B3k0kv9KfqwrRYRjEwak7Ie51zWj4+uyjidjtYo89DUfWhsvw6EZ+tQH7tgGRttXhiNVcztLtp133FlM3jFoecetxbbT9jvGH0/x4u43bPsls8usVFHG0alWj0FTa0tu3NH8iP86Q6fd/52x8ibhnzEdY6fj6/K+B31NHdO3SsbH6eoWxhhTOj5r29VdQ+5YZOWLxeFi/g/d7ZIZS8eTyRi71137wNOuVISuNGGMvzpDnvcQnoNiY9qyxtHpPvoZqvBYvM/aFKQ8Sda5Pcd5PxwfvjDtfs6H3pux7U1Tbqzdwpg/enZq2q3CM7n3/bqvUwnviIzZj5ckiZcqOfPX7m+4X6rk59F9tA+gK2QsWRGjw/MBivlXBgAAACtCpw4AACAB7Z1jn3MKtF4u1ShTqzRrLBveDhcHvkmnSRvjx6l7Bk+Fmy/x8qdPeLe9mFVXxZDLzwPT/mV8nd7db7IiyqwVJjohb8SZZ7vwvYQlTm7yVs6Y7LEV3Ltkdrv73Vo14/4H1dUY/CjWGP2zVrvs9tHIVFd3CEuNlI5XbXtxgyvDsPrYx7b9xZ0j3j4XHyvZ9pWH6w8BaWeE3Ywir1Kg5U2mjSsXtX8iXtIkb6mQle6/JGJt4HMr8mfdKv5n7kqCDFbc759+r8b45b6mDrkoNms4lUauet5dGKt/rar6eM27raVTtDyJ9g3eXvCHfcWGXbU8Ym2i9AlX6gAAABJApw4AACABXSlxvmQ1hbJrbvovF6vsn5ZL7HlntcqlysqoH535M2hdtOpVsJbL42EF6pJc4q3JVXS9XK+XmEP9sIpETBhjPrX7iG2/MOkut9/3MzfFcM34em+f+a3t+fyyZsLGVsXIio/fOe8u0W+TyPX8Pn0//fez4MWP4665pVS17ePDLu5cso/QWakaxWZt5/8PKxHtiP97rrNX9bHLO/8o1+vobF5dLQP5aHx3+9Pus6yecJ9lGIXq3+1Y5Jp35mneyDbvjNdkNBAJ6mdZnXDnyYHKmnqb//9jMuPdxF9Hz6e1sehmuRz4yM2sPmBcO/dM1lavFNHEc3ClDgAAIAF06gAAABJQiBWmdbaKzorRYoUXJuOXI73LtToTtezPPI3Fn5VIEcOQztoNZ7neNPjJVe+2LmIMR2eIarHJU1MrL9abt5BwTNY++lisKGZo5BUXLcztdPe3Kz7uFTeGr9n28TObbHvX+AXb1ujSGGPmL69b9nl1hmxIZ8zqrNRZiXVXzXi7mNntess9tx7/wGn/79Htf3zZ1FPUGa+9QqPMZoskx54r7+s08ppZr9NzWjjDsytDkYLjr3S4KHCncKUOAAAgAXTqAAAAEkCnDgAAIAGtH1PXwNTecIWJm3Ss3UAl3+LusfF14WPe/d74uPh0dh3vd9t5NxDn+mZXXX569x3Gt/KK5v1Ay368uPsN29byJmuCqiGx8iJZ8uyTNW7uqfVHws2NMf6YwMOvPug9tvG1/7Xtiz/8E3mkd8fU6XgyY+JjxXS7cBu9HT5fHosj9T8/HTeXtY+Og4s9V/h8up2O99vy9QvePhNDF237P48+ZNv5/mr1t7zjzlo5Hq1TY9t6egwd8ivQ+Dyu1AEAACSATh0AAEACWh+/NnAZUmNSjS/DRX+9l1kScy41Ug6jz8/rbqdlSBbuWl33/tDsw5tte2biVtvu51UjVkIjzzc21y9vMmUe8PbZJKVCZr49b9u6uoOuVGGMH5MqL1bdHT/OWMy6+eAl1zaXvH3OS+SaShmTvKU5GtnuXKlk2/u2/tbbrlzaaNtaBiUrSo1FwLXt9VelqQ37t7XEyl9tdAt9H1twi5OXZzd6+3iRK2VMViRvRNlsSZO8+iIybfUKCCgMrtQBAAAkgE4dAABAAgqxooTyF1POF8XGZMWnMRqlGrM6+pgfs6YRsXXL1CEXs+ps1XDm6uy4W11AV22YkhmzZq//3DrLNvaaa866/200VjXGmM/fP2HbG42b1Tr3zB7bvjAZznHk52EldNH7g+Z+7zGNYyd2uxmmB0tuu6ywc8t41e0vM1Q1PtWI1Rg/ZtX2wbPuNec/9Fe6YJZr+zUbi3Yqvu0JRK7J4kodAABAAujUAQAAJKA78etoMAstcilYo9jq47f6D04vX9S3OjG47DahgYprE6s1R+NTne2aRbfTWNQYY4bOuNnLa09WbXuTKdn2qald3j7bZLu5nSV5xP38zI7Hj2dBYtbZcfczmMqs1iLQ2aJzxv+d1Th2S6m64uc+Vy3VbetzvfTbP4vur9Ews1p7W99HrugLXKkDAABIAJ06AACABNCpAwAASEB3xtQ1Mp26Q1OwWRGidfKOo4sJx63Nb5UxjpPBMgAxubZzr3PiO1nbF/dnIyz/UnSxVR9CWjqkbNZFt1upvM/VjVIleT8bAAhxpQ4AACABdOoAAAASULgVJfpSVokXFl5eViOlU1LTa+87FisSN/b2Z6CrNhjTWBkRVn4AGseVOgAAgATQqQMAAEjALTdu3Fh+aQYAAAAUGlfqAAAAEkCnDgAAIAF06gAAABJApw4AACABdOoAAAASQKcOAAAgAXTqAAAAEkCnDgAAIAF06gAAABLwfy3Fzbhtdy/MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print some sample \n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "time_bins = 5\n",
    "idx = 1000\n",
    "\n",
    "events, target = dataset[idx]\n",
    "\n",
    "frame_transform = tonic_transforms.ToFrame(\n",
    "    sensor_size=sensor_size,\n",
    "    n_time_bins=time_bins\n",
    ")\n",
    "\n",
    "frames = frame_transform(events)\n",
    "\n",
    "def plot_frames(frames):\n",
    "    fig, axes = plt.subplots(1, len(frames))\n",
    "    for axis, frame in zip(axes, frames):\n",
    "        axis.imshow(frame[1] - frame[0])\n",
    "        axis.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4c10d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a transform to make sure only positive events are used\n",
    "class OnlyPositve(object):\n",
    "    def __call__(self, frames):\n",
    "        return frames[:, 1:2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4f2749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform compose, train and test datasets + loaders, settings\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "time_bin_width_us = 1000\n",
    "batch_size = 16\n",
    "\n",
    "frame_transform = tonic_transforms.Compose([\n",
    "    tonic_transforms.ToFrame(sensor_size=sensor_size, time_window=time_bin_width_us),\n",
    "    OnlyPositve()\n",
    "])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to=data_path, transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to=data_path, transform=frame_transform, train=False)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fda168e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames shape: torch.Size([16, 312, 1, 34, 34])\n",
      "targets shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "frames, targets = next(iter(trainloader))\n",
    "print(\"frames shape:\", frames.shape)  # [batch, time, polarity, H, W]\n",
    "print(\"targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479265ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumPooling2D(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            kernel_size: int,\n",
    "            stride: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # get input dimensions\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        x_unfold = F.unfold(\n",
    "            x,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride\n",
    "        )\n",
    "        #[batch, channels, window_area, num_windows]\n",
    "\n",
    "        window_area = self.kernel_size ** 2\n",
    "        num_windows = x_unfold.shape[-1]\n",
    "        x_unfold = x_unfold.view(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            window_area,\n",
    "            num_windows\n",
    "        )\n",
    "\n",
    "        pooled_sums = x_unfold.sum(dim=2)\n",
    "\n",
    "        height_out = (height - self.kernel_size) // self.stride + 1\n",
    "        width_out = (width - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        pooled_sums_2d = pooled_sums.view(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            height_out,\n",
    "            width_out\n",
    "        )\n",
    "\n",
    "        return pooled_sums_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a1642a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 3, 6, 6])\n",
      "output shape: torch.Size([2, 3, 3, 3])\n",
      "input: tensor([[[[  0.,   1.,   2.,   3.,   4.,   5.],\n",
      "          [  6.,   7.,   8.,   9.,  10.,  11.],\n",
      "          [ 12.,  13.,  14.,  15.,  16.,  17.],\n",
      "          [ 18.,  19.,  20.,  21.,  22.,  23.],\n",
      "          [ 24.,  25.,  26.,  27.,  28.,  29.],\n",
      "          [ 30.,  31.,  32.,  33.,  34.,  35.]],\n",
      "\n",
      "         [[ 36.,  37.,  38.,  39.,  40.,  41.],\n",
      "          [ 42.,  43.,  44.,  45.,  46.,  47.],\n",
      "          [ 48.,  49.,  50.,  51.,  52.,  53.],\n",
      "          [ 54.,  55.,  56.,  57.,  58.,  59.],\n",
      "          [ 60.,  61.,  62.,  63.,  64.,  65.],\n",
      "          [ 66.,  67.,  68.,  69.,  70.,  71.]],\n",
      "\n",
      "         [[ 72.,  73.,  74.,  75.,  76.,  77.],\n",
      "          [ 78.,  79.,  80.,  81.,  82.,  83.],\n",
      "          [ 84.,  85.,  86.,  87.,  88.,  89.],\n",
      "          [ 90.,  91.,  92.,  93.,  94.,  95.],\n",
      "          [ 96.,  97.,  98.,  99., 100., 101.],\n",
      "          [102., 103., 104., 105., 106., 107.]]],\n",
      "\n",
      "\n",
      "        [[[108., 109., 110., 111., 112., 113.],\n",
      "          [114., 115., 116., 117., 118., 119.],\n",
      "          [120., 121., 122., 123., 124., 125.],\n",
      "          [126., 127., 128., 129., 130., 131.],\n",
      "          [132., 133., 134., 135., 136., 137.],\n",
      "          [138., 139., 140., 141., 142., 143.]],\n",
      "\n",
      "         [[144., 145., 146., 147., 148., 149.],\n",
      "          [150., 151., 152., 153., 154., 155.],\n",
      "          [156., 157., 158., 159., 160., 161.],\n",
      "          [162., 163., 164., 165., 166., 167.],\n",
      "          [168., 169., 170., 171., 172., 173.],\n",
      "          [174., 175., 176., 177., 178., 179.]],\n",
      "\n",
      "         [[180., 181., 182., 183., 184., 185.],\n",
      "          [186., 187., 188., 189., 190., 191.],\n",
      "          [192., 193., 194., 195., 196., 197.],\n",
      "          [198., 199., 200., 201., 202., 203.],\n",
      "          [204., 205., 206., 207., 208., 209.],\n",
      "          [210., 211., 212., 213., 214., 215.]]]])\n",
      "output: tensor([[[[ 14.,  22.,  30.],\n",
      "          [ 62.,  70.,  78.],\n",
      "          [110., 118., 126.]],\n",
      "\n",
      "         [[158., 166., 174.],\n",
      "          [206., 214., 222.],\n",
      "          [254., 262., 270.]],\n",
      "\n",
      "         [[302., 310., 318.],\n",
      "          [350., 358., 366.],\n",
      "          [398., 406., 414.]]],\n",
      "\n",
      "\n",
      "        [[[446., 454., 462.],\n",
      "          [494., 502., 510.],\n",
      "          [542., 550., 558.]],\n",
      "\n",
      "         [[590., 598., 606.],\n",
      "          [638., 646., 654.],\n",
      "          [686., 694., 702.]],\n",
      "\n",
      "         [[734., 742., 750.],\n",
      "          [782., 790., 798.],\n",
      "          [830., 838., 846.]]]])\n"
     ]
    }
   ],
   "source": [
    "# test the pooling layer\n",
    "x = torch.arange(2*3*6*6).view(2, 3, 6, 6).float()\n",
    "sum_pool = SumPooling2D(kernel_size=2, stride=2)\n",
    "output = sum_pool(x)\n",
    "\n",
    "print(\"input shape:\", x.shape)\n",
    "print(\"output shape:\", output.shape)\n",
    "print(\"input:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3ef2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            num_classes: int,\n",
    "            kernel_size_conv: int,\n",
    "            kernel_size_pool: int,\n",
    "            stride_pool: int,\n",
    "            pool_threshold: float,\n",
    "            neuron_threshold: float,\n",
    "            neuron_reset_value: float,\n",
    "            decay: float\n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=kernel_size_conv,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.sum_pool = SumPooling2D(\n",
    "            kernel_size=kernel_size_pool,\n",
    "            stride=stride_pool\n",
    "        )\n",
    "        self.neuron_threshold = neuron_threshold\n",
    "        self.pool_threshold = pool_threshold\n",
    "        self.decay = decay\n",
    "        self.neuron_reset_value = neuron_reset_value\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        batch_size, num_steps, _, height, width = input_sequence.shape\n",
    "        device = input_sequence.device\n",
    "\n",
    "        conv_out = self.conv(input_sequence[:,0])\n",
    "        _, num_classes, conv_h, conv_w = conv_out.shape\n",
    "\n",
    "        membrane = torch.zeros(batch_size, num_classes, conv_h, conv_w, device=device)\n",
    "        spike_trains = []\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            #convolution\n",
    "            conv_out = self.conv(input_sequence[:,t])\n",
    "\n",
    "            #membrane update\n",
    "            membrane = membrane - self.decay + conv_out\n",
    "\n",
    "            #pooling for output spikes\n",
    "            pooled_sums = self.sum_pool(membrane)\n",
    "            spikes = (pooled_sums >= self.pool_threshold)\n",
    "            spike_trains.append(spikes)\n",
    "\n",
    "            #membrane thresholding\n",
    "            reset_mask = membrane >= self.neuron_threshold\n",
    "            membrane = membrane.masked_fill(reset_mask, self.neuron_reset_value)\n",
    "\n",
    "        spike_trains = torch.stack(spike_trains, dim=0)\n",
    "        spike_trains = spike_trains.permute(1, 0, 2, 3, 4)\n",
    "        # [batch, time, num_classes, height, width]\n",
    "        class_scores = spike_trains.sum(dim=(1,3,4)).float()\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "190d2e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d02b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    in_channels=1,\n",
    "    out_channels=10,              # or just num_classes=10 if out_channels unused\n",
    "    num_classes=10,\n",
    "    kernel_size_conv=3,\n",
    "    kernel_size_pool=2,\n",
    "    stride_pool=2,\n",
    "    pool_threshold=3.0,\n",
    "    neuron_threshold=1.0,\n",
    "    neuron_reset_value=0.0,\n",
    "    decay=0.95\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for frames, targets in train_loader:\n",
    "    frames, targets = frames.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    class_scores = model(frames)  # [batch, num_classes]\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(class_scores, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item() * frames.size(0)\n",
    "    # Compute predictions\n",
    "    preds = class_scores.argmax(dim=1)\n",
    "    total_correct += (preds == targets).sum().item()\n",
    "    total_samples += frames.size(0)\n",
    "\n",
    "print(f\"Epoch 1 finished: Loss = {total_loss / total_samples:.4f}, \"\n",
    "      f\"Accuracy = {total_correct / total_samples:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-conv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
